{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "02698124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data = pd.read_pickle(\"ass2.pickle\")\n",
    "train = data[\"train\"].to_numpy()\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "test = data[\"test\"].to_numpy()\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "dev = data[\"dev\"].to_numpy()\n",
    "X_dev = dev[:, :-1]\n",
    "y_dev = dev[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246a239",
   "metadata": {},
   "source": [
    "Preliminary data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91ad1972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of attributes: 41\n",
      "\n",
      "number of samples:\n",
      "\ttrain: 40533, test: 13512, dev: 13512\n",
      "\n",
      "number of nulls in train: 0\n",
      "number of nulls in test: 0\n",
      "number of nulls in dev: 0\n",
      "\n",
      "percentage of each category in the training data (check for balance):\n",
      "\n",
      "{2: 0.6595613450768509, 1: 0.2438013470505514, 0: 0.09663730787259764}\n",
      "\n",
      "percentage of each category in the dev data (check for balance):\n",
      "\n",
      "{2: 0.6563795145056246, 1: 0.2496299585553582, 0: 0.09399052693901717}\n",
      "\n",
      "percentage of each category in the dev data (check for balance):\n",
      "\n",
      "{2: 0.6564535227945529, 0: 0.09339846062759029, 1: 0.25014801657785674}\n",
      "\n",
      "categories: [2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of attributes: {X_train.shape[1] - 1}\\n\")\n",
    "print(f\"number of samples:\\n\\ttrain: {X_train.shape[0]}, test: {X_test.shape[0]}, dev: {X_dev.shape[0]}\\n\")\n",
    "\n",
    "train_num_of_nulls = np.sum(np.isnan(train))\n",
    "print(f\"number of nulls in train: {train_num_of_nulls}\")\n",
    "\n",
    "test_num_of_nulls = np.sum(np.isnan(test))\n",
    "print(f\"number of nulls in test: {test_num_of_nulls}\")\n",
    "\n",
    "dev_num_of_nulls = np.sum(np.isnan(dev))\n",
    "print(f\"number of nulls in dev: {dev_num_of_nulls}\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_dev_scaled = scaler.fit_transform(X_dev)\n",
    "\n",
    "# Merge the train and dev datasets\n",
    "X_train_and_dev_scaled = np.vstack((X_train_scaled, X_dev_scaled))\n",
    "y_train_and_dev = np.hstack((y_train, y_dev))\n",
    "\n",
    "percentage_of_each_category = {category: count/len(y_train) for category, count in Counter(y_train).items()}\n",
    "print(\"\\npercentage of each category in the training data (check for balance):\\n\")\n",
    "print(percentage_of_each_category)\n",
    "\n",
    "dev_percentage_of_each_category = {category: count/len(y_dev) for category, count in Counter(y_dev).items()}\n",
    "print(\"\\npercentage of each category in the dev data (check for balance):\\n\")\n",
    "print(dev_percentage_of_each_category)\n",
    "\n",
    "test_percentage_of_each_category = {category: count/len(y_test) for category, count in Counter(y_test).items()}\n",
    "print(\"\\npercentage of each category in the dev data (check for balance):\\n\")\n",
    "print(test_percentage_of_each_category)\n",
    "\n",
    "categories = list(percentage_of_each_category.keys())\n",
    "print(f\"\\ncategories: {categories}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ccd7a",
   "metadata": {},
   "source": [
    "-------------------------------------- flow --------------------------------------\n",
    "\n",
    "1. Running the models on the original training+dev data (unbalanced) with default hyperparameters\n",
    "2. Running the models on the original training+dev data (unbalanced) with optimized hyperparameters\n",
    "3. Running the models on the original training data, testing their roc auc scores on the dev data to determine every model's amount of say in the ensambles\n",
    "4. Testing all combinations of models in search for the best ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b604",
   "metadata": {},
   "source": [
    "Running the models on the original training data (unbalanced) with default hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6565239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.23      0.26      1262\n",
      "           1       0.63      0.52      0.57      3380\n",
      "           2       0.80      0.88      0.84      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.57      0.54      0.56     13512\n",
      "weighted avg       0.71      0.73      0.72     13512\n",
      "\n",
      "roc auc = 0.7461567089445434\n",
      "\n",
      "Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.24      0.23      1262\n",
      "           1       0.61      0.59      0.60      3380\n",
      "           2       0.82      0.82      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.55      0.55      0.55     13512\n",
      "weighted avg       0.71      0.71      0.71     13512\n",
      "\n",
      "roc auc = 0.6639066160300829\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.06      0.11      1262\n",
      "           1       0.78      0.65      0.71      3380\n",
      "           2       0.81      0.96      0.88      8870\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.70      0.56      0.57     13512\n",
      "weighted avg       0.77      0.80      0.76     13512\n",
      "\n",
      "roc auc = 0.8496277453556257\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1262\n",
      "           1       0.55      0.03      0.05      3380\n",
      "           2       0.66      0.99      0.79      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "roc auc = 0.6107986649820837\n",
      "\n",
      "Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.00      0.00      1262\n",
      "           1       0.75      0.50      0.60      3380\n",
      "           2       0.76      0.96      0.85      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.63      0.49      0.48     13512\n",
      "weighted avg       0.72      0.76      0.71     13512\n",
      "\n",
      "roc auc = 0.8018114674360349\n",
      "\n",
      "Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.11      0.12      1262\n",
      "           1       0.36      0.21      0.27      3380\n",
      "           2       0.69      0.82      0.75      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.39      0.38      0.38     13512\n",
      "weighted avg       0.56      0.60      0.57     13512\n",
      "\n",
      "roc auc = 0.5943070725751834\n",
      "\n",
      "\n",
      "Best model when fit to the original data (unbalanced) with default hyperparameters:\n",
      "\tRandom Forest\n",
      "roc auc: 0.8496277453556257\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "            'KNN': KNeighborsClassifier,\n",
    "            'Decision Tree': DecisionTreeClassifier,\n",
    "            'Random Forest': RandomForestClassifier,\n",
    "            'Logistic Regression': LogisticRegression, \n",
    "            'Gradient Boosting': GradientBoostingClassifier,\n",
    "            'Gaussian Naiive Bayes': GaussianNB,            \n",
    "        }\n",
    "\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_roc_auc = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "\n",
    "    # fit the model on the train_and_dev data\n",
    "    clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "        \n",
    "    # performance report\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "        \n",
    "    # roc auc score\n",
    "    y_pred = model.predict_proba(X_test_scaled)\n",
    "    model_roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovo', )\n",
    "    print(f\"roc auc = {model_roc_auc}\\n\")\n",
    "    \n",
    "    if (model_roc_auc > best_roc_auc):\n",
    "        best_roc_auc = model_roc_auc\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "                \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with default hyperparameters:\\n\\t{best_model_name}\")\n",
    "print(f\"roc auc: {best_roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57200ee0",
   "metadata": {},
   "source": [
    "Running the models on the original training data (unbalanced) with optimized hyperparameters:\n",
    "\n",
    "When using 'roc_auc' as the scoring metric in GridSearchCV, it means that the grid search will evaluate different parameter combinations based on how well they maximize roc auc value. The goal is to find the parameter combination that yields the highest roc auc score, indicating the best fit of the model to the data.\n",
    "\n",
    "We give weights to each class based on its percentage of the training set (the more it appears in the training data, the lower is it's weight). This helps offset the inbalance in the data a bit without undersampling (to not lose precious data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc51a42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "{'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}\n",
      "best combination chosen:  {'algorithm': 'ball_tree', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.31      0.28      1262\n",
      "           1       0.63      0.47      0.54      3380\n",
      "           2       0.80      0.85      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.56      0.55      0.55     13512\n",
      "weighted avg       0.71      0.71      0.70     13512\n",
      "\n",
      "roc auc = 0.7151836820976949\n",
      "\n",
      "Decision Tree:\n",
      "{'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10], 'criterion': ['gini', 'entropy']}\n",
      "best combination chosen:  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.26      0.25      1262\n",
      "           1       0.62      0.63      0.62      3380\n",
      "           2       0.83      0.82      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.56      0.57      0.56     13512\n",
      "weighted avg       0.72      0.72      0.72     13512\n",
      "\n",
      "roc auc = 0.6755157464848928\n",
      "\n",
      "Random Forest:\n",
      "{'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
      "best combination chosen:  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.09      0.15      1262\n",
      "           1       0.80      0.64      0.71      3380\n",
      "           2       0.81      0.96      0.88      8870\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.73      0.56      0.58     13512\n",
      "weighted avg       0.78      0.80      0.77     13512\n",
      "\n",
      "roc auc = 0.8542599545901455\n",
      "\n",
      "Logistic Regression:\n",
      "{'C': [0.1, 1.0, 10.0], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "            'KNN': {\n",
    "                    'n_neighbors': [3, 5, 7],\n",
    "                    'weights': ['uniform', 'distance'],\n",
    "                    'algorithm': ['ball_tree', 'kd_tree']\n",
    "                    },\n",
    "            'Decision Tree': {\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10],\n",
    "                                'criterion': ['gini', 'entropy']\n",
    "                            },\n",
    "            'Random Forest': {\n",
    "                                'n_estimators': [100, 200, 300],\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10]\n",
    "                            },\n",
    "            'Logistic Regression': {\n",
    "                                'C': [0.1, 1.0, 10.0],\n",
    "                                'penalty': ['l1', 'l2'],\n",
    "                                'solver': ['liblinear', 'saga']\n",
    "                                    }, \n",
    "            'Gradient Boosting': {\n",
    "                                'n_estimators': [50, 100, 150],\n",
    "                                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                                'max_depth': [3, 5, 7]\n",
    "                                },\n",
    "            'Gaussian Naiive Bayes': {'var_smoothing': [1e-9, 1e-8, 1e-7]},            \n",
    "        }\n",
    "\n",
    "class_weights = {\n",
    "    0: 1 / percentage_of_each_category[0], \n",
    "    1: 1 / percentage_of_each_category[1], \n",
    "    2: 1 / percentage_of_each_category[2] \n",
    "} # inversely proportional to the category's percentage in the dataset\n",
    "\n",
    "# give every sample it's weight\n",
    "sample_weights = np.array([class_weights[label] for label in y_train_and_dev])\n",
    "\n",
    "models_iteration_2 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_roc_auc = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        grid_search.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    else:\n",
    "        grid_search.fit(X_train_and_dev_scaled, y_train_and_dev, sample_weight=sample_weights)\n",
    "   \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(\"best combination chosen: \", best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "\n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    else:\n",
    "        clf = model.fit(X_train_and_dev_scaled, y_train_and_dev, sample_weight=sample_weights)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "\n",
    "    y_pred = model.predict_proba(X_test_scaled)\n",
    "    model_roc_auc = roc_auc_score(y_test, y_pred, multi_class='ovo')\n",
    "    print(f\"roc auc = {model_roc_auc}\\n\")\n",
    "    \n",
    "    if (model_roc_auc > best_roc_auc):\n",
    "        best_roc_auc = model_roc_auc\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "    models_iteration_2[name] = model\n",
    "                \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with optimized hyperparameters:\\n\\t{best_model_name}\")\n",
    "print(f\"roc auc: {best_roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6a696",
   "metadata": {},
   "source": [
    "Fit the models on the training set with default hyperparameters and test roc auc on the dev set.\n",
    "We save the roc auc of each model on the dev set. We later use this accuracy to set the \"amount of say\" of each model in its ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e0365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_iteration_3 = {}\n",
    "training_roc_auc_iteration_3 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_roc_auc = 0\n",
    "\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "\n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        grid_search.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "    \n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        clf = model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        clf = model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "        \n",
    "    str_report = classification_report(y_dev, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_dev_scaled)\n",
    "    model_roc_auc = roc_auc_score(y_dev, y_pred, multi_class='ovo')\n",
    "    print(f\"roc auc = {model_roc_auc}\\n\")\n",
    "    \n",
    "    if (model_roc_auc > best_roc_auc):\n",
    "        best_roc_auc = model_roc_auc\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "                \n",
    "    models_iteration_3[name] = clf\n",
    "    training_roc_auc_iteration_3[name] = model_roc_auc\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with default hyperparameters:\\n\\t{best_model_name}\")\n",
    "print(f\"roc auc: {best_roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24ea2b",
   "metadata": {},
   "source": [
    "Creating all possible combinations of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "\n",
    "for r in range(2, len(models_iteration_3) + 1):\n",
    "    subsets = itertools.combinations(models_iteration_3.items(), r)\n",
    "    # subsets is a list of tuples (name, model)\n",
    "    for subset in subsets:\n",
    "        ensemble = {}\n",
    "        for name, model in subset:\n",
    "            ensemble[name] = model\n",
    "        ensembles.append(ensemble)\n",
    "                \n",
    "print(f\"num of subsets = {len(ensembles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7b0e4",
   "metadata": {},
   "source": [
    "Testing all possible combinations of models as ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b1e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_ensamble = None\n",
    "best_ensamble_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for ensemble in ensembles:\n",
    "    ensemble_name = \", \".join([name for name in ensemble.keys()]) # join all the model names    \n",
    "    print(f\"{ensemble_name}:\") \n",
    "    \n",
    "    preds = np.zeros((len(y_test), len(categories)))\n",
    "    # sum of the roc_aucs of this ensamble's models\n",
    "    sum_of_roc_aucs = np.sum([training_roc_auc_iteration_3[name] for name in ensemble.keys()])\n",
    "    for name, model in ensemble.items():   \n",
    "        pred = model.predict_proba(X_test_scaled)\n",
    "        # multiply the prediction by the \"amount of say\" of this model\n",
    "        pred *= training_roc_auc_iteration_3[name] / sum_of_roc_aucs\n",
    "#         pred /= len(ensemble)\n",
    "        preds += pred\n",
    "    \n",
    "    # Determine the class with the maximum overall weighted votes\n",
    "    majority_vote = (np.argmax(preds, axis=1)).astype(int)\n",
    "        \n",
    "    str_report = classification_report(y_test, majority_vote, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    model_roc_auc = roc_auc_score(y_dev, preds, multi_class='ovo')\n",
    "    print(f\"roc auc = {model_roc_auc}\\n\")\n",
    "    \n",
    "    if (model_roc_auc > best_roc_auc):\n",
    "        best_roc_auc = model_roc_auc\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest ensemble when fit to the original data (unbalanced) with optimized hyperparameters:\\n\\t{best_ensemble_name}\")\n",
    "print(f\"roc auc: {best_roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8229e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
