{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02698124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data = pd.read_pickle(\"ass2.pickle\")\n",
    "train = data[\"train\"].to_numpy()\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "test = data[\"test\"].to_numpy()\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "dev = data[\"dev\"].to_numpy()\n",
    "X_dev = dev[:, :-1]\n",
    "y_dev = dev[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246a239",
   "metadata": {},
   "source": [
    "Preliminary data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ad1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of attributes: 41\n",
      "\n",
      "number of samples:\n",
      "\ttrain: 40533, test: 13512, dev: 13512\n",
      "\n",
      "number of nulls in train: 0\n",
      "number of nulls in test: 0\n",
      "number of nulls in dev: 0\n",
      "\n",
      "percentage of each category in the training data (check for balance):\n",
      "\n",
      "{2: 0.6595613450768509, 1: 0.2438013470505514, 0: 0.09663730787259764}\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of attributes: {X_train.shape[1] - 1}\\n\")\n",
    "print(f\"number of samples:\\n\\ttrain: {X_train.shape[0]}, test: {X_test.shape[0]}, dev: {X_dev.shape[0]}\\n\")\n",
    "\n",
    "train_num_of_nulls = np.sum(np.isnan(train))\n",
    "print(f\"number of nulls in train: {train_num_of_nulls}\")\n",
    "\n",
    "test_num_of_nulls = np.sum(np.isnan(test))\n",
    "print(f\"number of nulls in test: {test_num_of_nulls}\")\n",
    "\n",
    "dev_num_of_nulls = np.sum(np.isnan(dev))\n",
    "print(f\"number of nulls in dev: {dev_num_of_nulls}\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_dev_scaled = scaler.fit_transform(X_dev)\n",
    "\n",
    "# Merge the train and dev datasets\n",
    "X_train_and_dev_scaled = np.vstack((X_train_scaled, X_dev_scaled))\n",
    "y_train_and_dev = np.hstack((y_train, y_dev))\n",
    "\n",
    "percentage_of_each_category = {category: count/len(y_train) for category, count in Counter(y_train).items()}\n",
    "print(\"\\npercentage of each category in the training data (check for balance):\\n\")\n",
    "print(percentage_of_each_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1098a1",
   "metadata": {},
   "source": [
    "1. Running the models on the original training data (unbalanced) with default hyperparameters\n",
    "2. Running the models on the original training data (unbalanced) with optimized hyperparameters\n",
    "3. Running the models on the balanced training data with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Running the models on the original training data (unbalanced) with default hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6565239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.23      0.26      1262\n",
      "           1       0.63      0.52      0.57      3380\n",
      "           2       0.80      0.88      0.84      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.57      0.54      0.56     13512\n",
      "weighted avg       0.71      0.73      0.72     13512\n",
      "\n",
      "Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.24      0.24      1262\n",
      "           1       0.62      0.60      0.61      3380\n",
      "           2       0.83      0.83      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.56      0.56      0.56     13512\n",
      "weighted avg       0.72      0.72      0.72     13512\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.07      0.12      1262\n",
      "           1       0.78      0.65      0.71      3380\n",
      "           2       0.81      0.96      0.88      8870\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.71      0.56      0.57     13512\n",
      "weighted avg       0.77      0.80      0.76     13512\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1262\n",
      "           1       0.55      0.03      0.05      3380\n",
      "           2       0.66      0.99      0.79      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1262\n",
      "           1       0.70      0.42      0.53      3380\n",
      "           2       0.74      0.95      0.83      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.48      0.46      0.45     13512\n",
      "weighted avg       0.66      0.73      0.68     13512\n",
      "\n",
      "Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.00      0.00      1262\n",
      "           1       0.75      0.50      0.60      3380\n",
      "           2       0.76      0.96      0.85      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.63      0.49      0.48     13512\n",
      "weighted avg       0.72      0.76      0.71     13512\n",
      "\n",
      "Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.11      0.12      1262\n",
      "           1       0.36      0.21      0.27      3380\n",
      "           2       0.69      0.82      0.75      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.39      0.38      0.38     13512\n",
      "weighted avg       0.56      0.60      0.57     13512\n",
      "\n",
      "\n",
      "Best model when fit to the original data (unbalanced) with default hyperparameters:\n",
      "\tRandom Forest\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "            'KNN': KNeighborsClassifier,\n",
    "            'Decision Tree': DecisionTreeClassifier,\n",
    "            'Random Forest': RandomForestClassifier,\n",
    "            'Logistic Regression': LogisticRegression, \n",
    "            'Adaptive Boosting': AdaBoostClassifier,\n",
    "            'Gradient Boosting': GradientBoostingClassifier,\n",
    "            'Gaussian Naiive Bayes': GaussianNB,            \n",
    "        }\n",
    "\n",
    "hyperparameters = {\n",
    "            'KNN': {\n",
    "                    'n_neighbors': [3, 5, 7],\n",
    "                    'weights': ['uniform', 'distance'],\n",
    "                    'algorithm': ['ball_tree', 'kd_tree']\n",
    "                    },\n",
    "            'Decision Tree': {\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10],\n",
    "                                'criterion': ['gini', 'entropy']\n",
    "                            },\n",
    "            'Random Forest': {\n",
    "                                'n_estimators': [100, 200, 300],\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10]\n",
    "                            },\n",
    "            'Logistic Regression': {\n",
    "                                'C': [0.1, 1.0, 10.0],\n",
    "                                'penalty': ['l1', 'l2'],\n",
    "                                'solver': ['liblinear', 'saga']\n",
    "                                    }, \n",
    "            'Adaptive Boosting': {\n",
    "                                'n_estimators': [50, 100, 150],\n",
    "                                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                                'base_estimator__max_depth': [1, 3, 5]\n",
    "                                },\n",
    "            'Gradient Boosting': {\n",
    "                                'n_estimators': [50, 100, 150],\n",
    "                                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                                'max_depth': [3, 5, 7]\n",
    "                                },\n",
    "            'Gaussian Naiive Bayes': {'var_smoothing': [1e-9, 1e-8, 1e-7]},            \n",
    "        }\n",
    "\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with default hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57200ee0",
   "metadata": {},
   "source": [
    "Running the models on the original training data (unbalanced) with optimized hyperparameters:\n",
    "\n",
    "* When using 'r2' as the scoring metric in GridSearchCV, it means that the grid search will evaluate different parameter combinations based on how well they maximize the R-squared value. The goal is to find the parameter combination that yields the highest R-squared score, indicating the best fit of the model to the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc51a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "{'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}\n",
      "{'weights': 'uniform', 'n_neighbors': 7, 'algorithm': 'ball_tree'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.16      0.22      1262\n",
      "           1       0.67      0.53      0.59      3380\n",
      "           2       0.79      0.92      0.85      8870\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.60      0.54      0.56     13512\n",
      "weighted avg       0.72      0.75      0.73     13512\n",
      "\n",
      "Decision Tree:\n",
      "{'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10], 'criterion': ['gini', 'entropy']}\n",
      "{'min_samples_split': 2, 'max_depth': 10, 'criterion': 'gini'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.03      0.05      1262\n",
      "           1       0.62      0.50      0.55      3380\n",
      "           2       0.76      0.91      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.53      0.48      0.48     13512\n",
      "weighted avg       0.67      0.72      0.69     13512\n",
      "\n",
      "Random Forest:\n",
      "{'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
      "{'n_estimators': 200, 'min_samples_split': 2, 'max_depth': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.07      0.12      1262\n",
      "           1       0.79      0.65      0.71      3380\n",
      "           2       0.81      0.96      0.88      8870\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.71      0.56      0.57     13512\n",
      "weighted avg       0.78      0.80      0.77     13512\n",
      "\n",
      "Logistic Regression:\n",
      "{'C': [0.1, 1.0, 10.0], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Unpacking the best_params dict into the model constructor\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n\u001b[1;32m---> 29\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_and_dev_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_and_dev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     32\u001b[0m dict_report \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1216\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1211\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1212\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1215\u001b[0m         )\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1224\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1221\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1223\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[1;32m-> 1224\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='r2', n_jobs=-1)\n",
    "    # grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    grid_search.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(\"best combination chosen: \", best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with optimized hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42bd4a9",
   "metadata": {},
   "source": [
    "Creating a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the balanced dataset\n",
    "oversampler = RandomOverSampler()\n",
    "X_train_and_dev_balanced, y_train_and_dev_balanced = oversampler.fit_resample(X_train_and_dev_scaled, y_train_and_dev)\n",
    "\n",
    "balanced_percentage_of_each_category = {category: count/len(y_train_and_dev_balanced) for category, count in Counter(y_train_and_dev_balanced).items()}\n",
    "print(\"\\npercentage of each category in the balanced training data (check for balance):\\n\")\n",
    "print(balanced_percentage_of_each_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18702f",
   "metadata": {},
   "source": [
    "Running the models on a balanced subset of the training data with optimized hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='r2', n_jobs=-1)\n",
    "    # grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    grid_search.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_balanced, y_train_and_dev_balanced)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to a balanced subset of the original data with optimized hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a0acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
