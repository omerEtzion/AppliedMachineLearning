{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02698124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data = pd.read_pickle(\"ass2.pickle\")\n",
    "train = data[\"train\"].to_numpy()\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "test = data[\"test\"].to_numpy()\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "dev = data[\"dev\"].to_numpy()\n",
    "X_dev = dev[:, :-1]\n",
    "y_dev = dev[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246a239",
   "metadata": {},
   "source": [
    "Preliminary data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91ad1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of attributes: 41\n",
      "\n",
      "number of samples:\n",
      "\ttrain: 40533, test: 13512, dev: 13512\n",
      "\n",
      "number of nulls in train: 0\n",
      "number of nulls in test: 0\n",
      "number of nulls in dev: 0\n",
      "\n",
      "percentage of each category in the training data (check for balance):\n",
      "\n",
      "{2: 0.6595613450768509, 1: 0.2438013470505514, 0: 0.09663730787259764}\n",
      "\n",
      "percentage of each category in the dev data (check for balance):\n",
      "\n",
      "{2: 0.6563795145056246, 1: 0.2496299585553582, 0: 0.09399052693901717}\n",
      "\n",
      "percentage of each category in the dev data (check for balance):\n",
      "\n",
      "{2: 0.6564535227945529, 0: 0.09339846062759029, 1: 0.25014801657785674}\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of attributes: {X_train.shape[1] - 1}\\n\")\n",
    "print(f\"number of samples:\\n\\ttrain: {X_train.shape[0]}, test: {X_test.shape[0]}, dev: {X_dev.shape[0]}\\n\")\n",
    "\n",
    "train_num_of_nulls = np.sum(np.isnan(train))\n",
    "print(f\"number of nulls in train: {train_num_of_nulls}\")\n",
    "\n",
    "test_num_of_nulls = np.sum(np.isnan(test))\n",
    "print(f\"number of nulls in test: {test_num_of_nulls}\")\n",
    "\n",
    "dev_num_of_nulls = np.sum(np.isnan(dev))\n",
    "print(f\"number of nulls in dev: {dev_num_of_nulls}\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_dev_scaled = scaler.fit_transform(X_dev)\n",
    "\n",
    "# Merge the train and dev datasets\n",
    "X_train_and_dev_scaled = np.vstack((X_train_scaled, X_dev_scaled))\n",
    "y_train_and_dev = np.hstack((y_train, y_dev))\n",
    "\n",
    "percentage_of_each_category = {category: count/len(y_train) for category, count in Counter(y_train).items()}\n",
    "print(\"\\npercentage of each category in the training data (check for balance):\\n\")\n",
    "print(percentage_of_each_category)\n",
    "\n",
    "dev_percentage_of_each_category = {category: count/len(y_dev) for category, count in Counter(y_dev).items()}\n",
    "print(\"\\npercentage of each category in the dev data (check for balance):\\n\")\n",
    "print(dev_percentage_of_each_category)\n",
    "\n",
    "test_percentage_of_each_category = {category: count/len(y_test) for category, count in Counter(y_test).items()}\n",
    "print(\"\\npercentage of each category in the dev data (check for balance):\\n\")\n",
    "print(test_percentage_of_each_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ccd7a",
   "metadata": {},
   "source": [
    "-------------------------------------- flow --------------------------------------\n",
    "\n",
    "1. Running the models on the original training data (unbalanced) with default hyperparameters\n",
    "2. Running the models on the original training data (unbalanced) with optimized hyperparameters\n",
    "3. Running the models on the balanced training data with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b604",
   "metadata": {},
   "source": [
    "Running the models on the original training data (unbalanced) with default hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6565239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.23      0.26      1262\n",
      "           1       0.63      0.52      0.57      3380\n",
      "           2       0.80      0.88      0.84      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.57      0.54      0.56     13512\n",
      "weighted avg       0.71      0.73      0.72     13512\n",
      "\n",
      "Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.25      0.24      1262\n",
      "           1       0.61      0.60      0.61      3380\n",
      "           2       0.83      0.82      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.56      0.56      0.56     13512\n",
      "weighted avg       0.72      0.71      0.72     13512\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.06      0.12      1262\n",
      "           1       0.78      0.65      0.71      3380\n",
      "           2       0.81      0.96      0.88      8870\n",
      "\n",
      "    accuracy                           0.80     13512\n",
      "   macro avg       0.71      0.56      0.57     13512\n",
      "weighted avg       0.78      0.80      0.76     13512\n",
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1262\n",
      "           1       0.55      0.03      0.05      3380\n",
      "           2       0.66      0.99      0.79      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.41      0.34      0.28     13512\n",
      "weighted avg       0.57      0.66      0.54     13512\n",
      "\n",
      "Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1262\n",
      "           1       0.70      0.42      0.53      3380\n",
      "           2       0.74      0.95      0.83      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.48      0.46      0.45     13512\n",
      "weighted avg       0.66      0.73      0.68     13512\n",
      "\n",
      "Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.00      0.00      1262\n",
      "           1       0.75      0.50      0.60      3380\n",
      "           2       0.76      0.96      0.85      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.63      0.49      0.48     13512\n",
      "weighted avg       0.72      0.76      0.71     13512\n",
      "\n",
      "Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.11      0.12      1262\n",
      "           1       0.36      0.21      0.27      3380\n",
      "           2       0.69      0.82      0.75      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.39      0.38      0.38     13512\n",
      "weighted avg       0.56      0.60      0.57     13512\n",
      "\n",
      "\n",
      "Best model when fit to the original data (unbalanced) with default hyperparameters:\n",
      "\tRandom Forest\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "            'KNN': KNeighborsClassifier,\n",
    "            'Decision Tree': DecisionTreeClassifier,\n",
    "            'Random Forest': RandomForestClassifier,\n",
    "            'Logistic Regression': LogisticRegression, \n",
    "            'Adaptive Boosting': AdaBoostClassifier,\n",
    "            'Gradient Boosting': GradientBoostingClassifier,\n",
    "            'Gaussian Naiive Bayes': GaussianNB,            \n",
    "        }\n",
    "\n",
    "hyperparameters = {\n",
    "            'KNN': {\n",
    "                    'n_neighbors': [3, 5, 7],\n",
    "                    'weights': ['uniform', 'distance'],\n",
    "                    'algorithm': ['ball_tree', 'kd_tree']\n",
    "                    },\n",
    "            'Decision Tree': {\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10],\n",
    "                                'criterion': ['gini', 'entropy']\n",
    "                            },\n",
    "            'Random Forest': {\n",
    "                                'n_estimators': [100, 200, 300],\n",
    "                                'max_depth': [None, 5, 10],\n",
    "                                'min_samples_split': [2, 5, 10]\n",
    "                            },\n",
    "            'Logistic Regression': {\n",
    "                                'C': [0.1, 1.0, 10.0],\n",
    "                                'penalty': ['l1', 'l2'],\n",
    "                                'solver': ['liblinear', 'saga']\n",
    "                                    }, \n",
    "            'Adaptive Boosting': {\n",
    "                                'n_estimators': [50, 100, 150],\n",
    "                                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                                'base_estimator__max_depth': [1, 3, 5]\n",
    "                                },\n",
    "            'Gradient Boosting': {\n",
    "                                'n_estimators': [50, 100, 150],\n",
    "                                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                                'max_depth': [3, 5, 7]\n",
    "                                },\n",
    "            'Gaussian Naiive Bayes': {'var_smoothing': [1e-9, 1e-8, 1e-7]},            \n",
    "        }\n",
    "\n",
    "models_iteration_1 = {}\n",
    "training_accuracies_iteration_1 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "        \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    models_iteration_1[name] = clf\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with default hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57200ee0",
   "metadata": {},
   "source": [
    "Running the models on the original training data (unbalanced) with optimized hyperparameters:\n",
    "\n",
    "* When using 'r2' as the scoring metric in GridSearchCV, it means that the grid search will evaluate different parameter combinations based on how well they maximize the R-squared value. The goal is to find the parameter combination that yields the highest R-squared score, indicating the best fit of the model to the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bc51a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "{'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mmodel_hyperparameters, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV object to your data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_and_dev_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_and_dev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     24\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_iteration_2 = {}\n",
    "accuracies_iteration_2 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='r2', n_jobs=-1)\n",
    "    # grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    grid_search.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(\"best combination chosen: \", best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    models_iteration_2[name] = clf\n",
    "    accuracies_iteration_2[name] = dict_report[\"accuracy\"]\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with optimized hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42bd4a9",
   "metadata": {},
   "source": [
    "Creating a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the balanced dataset\n",
    "oversampler = RandomOverSampler()\n",
    "X_train_and_dev_balanced, y_train_and_dev_balanced = oversampler.fit_resample(X_train_and_dev_scaled, y_train_and_dev)\n",
    "\n",
    "balanced_percentage_of_each_category = {category: count/len(y_train_and_dev_balanced) for category, count in Counter(y_train_and_dev_balanced).items()}\n",
    "print(\"\\npercentage of each category in the balanced training data (check for balance):\\n\")\n",
    "print(balanced_percentage_of_each_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18702f",
   "metadata": {},
   "source": [
    "Running the models on a balanced subset of the training data with optimized hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_iteration_3 = {}\n",
    "accuracies_iteration_3 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "    \n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='r2', n_jobs=-1)\n",
    "    # grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    grid_search.fit(X_train_and_dev_scaled, y_train_and_dev)\n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "\n",
    "    clf = model.fit(X_train_and_dev_balanced, y_train_and_dev_balanced)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    dict_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    models_iteration_3[name] = clf\n",
    "    accuracies_iteration_3[name] = dict_report[\"accuracy\"]\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to a balanced subset of the original data with optimized hyperparameters:\\n\\t{best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22738683",
   "metadata": {},
   "source": [
    "Fit the models on the training set with default hyperparameters and test accuracy on the dev set\n",
    "We gave weights to each class based on its percentage of the training set\n",
    "We saved the accuracy of each model on the dev set. We later use this accuracy to set the \"amount of say\" of each model in its ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db34e426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "{'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\etzio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\nTypeError: KNeighborsClassifier.fit() got an unexpected keyword argument 'sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mmodel_hyperparameters, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV object to your data\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     31\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\etzio\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\nTypeError: KNeighborsClassifier.fit() got an unexpected keyword argument 'sample_weight'\n"
     ]
    }
   ],
   "source": [
    "models_iteration_4 = {}\n",
    "training_accuracies_iteration_4 = {}\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "class_weights = {\n",
    "    0: 1 / percentage_of_each_category[0], \n",
    "    1: 1 / percentage_of_each_category[1], \n",
    "    2: 1 / percentage_of_each_category[2] \n",
    "} # inversely proportional to the category's percentage in the dataset\n",
    "\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "for name, model_class in models.items():\n",
    "    model = model_class()\n",
    "    print(f\"{name}:\")\n",
    "\n",
    "    model_hyperparameters = hyperparameters.get(name)\n",
    "    print(model_hyperparameters)\n",
    "    \n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=model_hyperparameters, cv=5, scoring='r2', n_jobs=-1)\n",
    "    # grid_search = RandomizedSearchCV(model, model_hyperparameters, n_iter=10, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        grid_search.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "        \n",
    "    \n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    # Unpacking the best_params dict into the model constructor\n",
    "    model = model_class(**best_params)\n",
    "    \n",
    "    if (name == 'KNN' or name == 'Gaussian Naiive Bayes'): # only these models don't take the argument sample_weight\n",
    "        clf = model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        clf = model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "    y_pred = clf.predict(X_dev_scaled)\n",
    "        \n",
    "    dict_report = classification_report(y_dev, y_pred, output_dict=True)\n",
    "    str_report = classification_report(y_dev, y_pred, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    models_iteration_4[name] = clf\n",
    "    training_accuracies_iteration_4[name] = dict_report[\"accuracy\"]\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        \n",
    "print(f\"\\nBest model when fit to the original data (unbalanced) with default hyperparameters:\\n\\t{best_model_name}\")\n",
    "print(f\"accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c43a5",
   "metadata": {},
   "source": [
    "Creating all possible combinations of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d94719b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of subsets = 120\n"
     ]
    }
   ],
   "source": [
    "ensembles = []\n",
    "\n",
    "for r in range(2, len(models_iteration_4) + 1):\n",
    "    subsets = itertools.combinations(models_iteration_4.items(), r)\n",
    "    # subsets is a list of tuples (name, model)\n",
    "    for subset in subsets:\n",
    "        ensemble = {}\n",
    "        for name, model in subset:\n",
    "            ensemble[name] = model\n",
    "        ensembles.append(ensemble)\n",
    "                \n",
    "print(f\"num of subsets = {len(ensembles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477aa73f",
   "metadata": {},
   "source": [
    "Testing all possible combinations of models as ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7e00075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN, Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.18      0.23      1262\n",
      "           1       0.52      0.71      0.60      3380\n",
      "           2       0.86      0.80      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.57      0.56      0.55     13512\n",
      "weighted avg       0.73      0.72      0.72     13512\n",
      "\n",
      "KNN, Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.08      0.14      1262\n",
      "           1       0.68      0.69      0.69      3380\n",
      "           2       0.83      0.93      0.88      8870\n",
      "\n",
      "    accuracy                           0.79     13512\n",
      "   macro avg       0.67      0.57      0.57     13512\n",
      "weighted avg       0.76      0.79      0.76     13512\n",
      "\n",
      "KNN, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.19      0.23      1262\n",
      "           1       0.38      0.65      0.48      3380\n",
      "           2       0.81      0.65      0.72      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.50      0.49      0.48     13512\n",
      "weighted avg       0.66      0.60      0.62     13512\n",
      "\n",
      "KNN, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.19      0.23      1262\n",
      "           1       0.37      0.64      0.47      3380\n",
      "           2       0.83      0.65      0.73      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.50      0.49      0.48     13512\n",
      "weighted avg       0.67      0.60      0.62     13512\n",
      "\n",
      "KNN, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.21      0.25      1262\n",
      "           1       0.41      0.59      0.49      3380\n",
      "           2       0.83      0.72      0.77      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.51      0.51      0.50     13512\n",
      "weighted avg       0.67      0.64      0.65     13512\n",
      "\n",
      "KNN, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.08      0.12      1262\n",
      "           1       0.50      0.61      0.55      3380\n",
      "           2       0.80      0.82      0.81      8870\n",
      "\n",
      "    accuracy                           0.70     13512\n",
      "   macro avg       0.54      0.50      0.50     13512\n",
      "weighted avg       0.68      0.70      0.68     13512\n",
      "\n",
      "Decision Tree, Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.17      1262\n",
      "           1       0.63      0.71      0.67      3380\n",
      "           2       0.84      0.89      0.87      8870\n",
      "\n",
      "    accuracy                           0.77     13512\n",
      "   macro avg       0.66      0.57      0.57     13512\n",
      "weighted avg       0.76      0.77      0.75     13512\n",
      "\n",
      "Decision Tree, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.24      0.26      1262\n",
      "           1       0.40      0.74      0.52      3380\n",
      "           2       0.86      0.60      0.71      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.52      0.53      0.50     13512\n",
      "weighted avg       0.69      0.60      0.62     13512\n",
      "\n",
      "Decision Tree, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.25      0.27      1262\n",
      "           1       0.40      0.74      0.52      3380\n",
      "           2       0.88      0.61      0.72      8870\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.52      0.53      0.50     13512\n",
      "weighted avg       0.70      0.61      0.63     13512\n",
      "\n",
      "Decision Tree, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.27      0.28      1262\n",
      "           1       0.43      0.72      0.54      3380\n",
      "           2       0.88      0.67      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.54      0.55      0.53     13512\n",
      "weighted avg       0.71      0.65      0.66     13512\n",
      "\n",
      "Decision Tree, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.09      0.14      1262\n",
      "           1       0.50      0.75      0.60      3380\n",
      "           2       0.85      0.77      0.81      8870\n",
      "\n",
      "    accuracy                           0.70     13512\n",
      "   macro avg       0.55      0.54      0.52     13512\n",
      "weighted avg       0.71      0.70      0.69     13512\n",
      "\n",
      "Random Forest, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.11      0.18      1262\n",
      "           1       0.44      0.75      0.55      3380\n",
      "           2       0.83      0.69      0.75      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.59      0.52      0.49     13512\n",
      "weighted avg       0.70      0.65      0.65     13512\n",
      "\n",
      "Random Forest, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.11      0.18      1262\n",
      "           1       0.43      0.78      0.56      3380\n",
      "           2       0.86      0.69      0.76      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.60      0.53      0.50     13512\n",
      "weighted avg       0.72      0.66      0.66     13512\n",
      "\n",
      "Random Forest, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.12      0.19      1262\n",
      "           1       0.48      0.76      0.59      3380\n",
      "           2       0.86      0.76      0.81      8870\n",
      "\n",
      "    accuracy                           0.70     13512\n",
      "   macro avg       0.61      0.54      0.53     13512\n",
      "weighted avg       0.73      0.70      0.69     13512\n",
      "\n",
      "Random Forest, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.05      0.09      1262\n",
      "           1       0.63      0.70      0.66      3380\n",
      "           2       0.82      0.89      0.85      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.65      0.55      0.54     13512\n",
      "weighted avg       0.74      0.76      0.73     13512\n",
      "\n",
      "Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.50      0.26      1262\n",
      "           1       0.44      0.63      0.52      3380\n",
      "           2       0.88      0.51      0.65      8870\n",
      "\n",
      "    accuracy                           0.54     13512\n",
      "   macro avg       0.50      0.54      0.48     13512\n",
      "weighted avg       0.70      0.54      0.58     13512\n",
      "\n",
      "Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.48      0.29      1262\n",
      "           1       0.45      0.72      0.55      3380\n",
      "           2       0.90      0.53      0.67      8870\n",
      "\n",
      "    accuracy                           0.57     13512\n",
      "   macro avg       0.52      0.58      0.50     13512\n",
      "weighted avg       0.72      0.57      0.60     13512\n",
      "\n",
      "Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.10      0.11      1262\n",
      "           1       0.29      0.38      0.33      3380\n",
      "           2       0.70      0.65      0.67      8870\n",
      "\n",
      "    accuracy                           0.53     13512\n",
      "   macro avg       0.38      0.37      0.37     13512\n",
      "weighted avg       0.54      0.53      0.53     13512\n",
      "\n",
      "Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.52      0.31      1262\n",
      "           1       0.47      0.70      0.56      3380\n",
      "           2       0.92      0.56      0.70      8870\n",
      "\n",
      "    accuracy                           0.59     13512\n",
      "   macro avg       0.53      0.59      0.52     13512\n",
      "weighted avg       0.74      0.59      0.63     13512\n",
      "\n",
      "Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.09      0.12      1262\n",
      "           1       0.27      0.40      0.33      3380\n",
      "           2       0.72      0.63      0.67      8870\n",
      "\n",
      "    accuracy                           0.52     13512\n",
      "   macro avg       0.38      0.38      0.37     13512\n",
      "weighted avg       0.55      0.52      0.53     13512\n",
      "\n",
      "Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.21      0.21      1262\n",
      "           1       0.45      0.81      0.58      3380\n",
      "           2       0.91      0.62      0.74      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.52      0.55      0.51     13512\n",
      "weighted avg       0.73      0.63      0.65     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.08      0.14      1262\n",
      "           1       0.57      0.73      0.64      3380\n",
      "           2       0.85      0.86      0.86      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.63      0.56      0.55     13512\n",
      "weighted avg       0.75      0.76      0.74     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24      1262\n",
      "           1       0.50      0.68      0.58      3380\n",
      "           2       0.86      0.80      0.83      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.57      0.55      0.55     13512\n",
      "weighted avg       0.72      0.71      0.71     13512\n",
      "\n",
      "KNN, Decision Tree, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24      1262\n",
      "           1       0.39      0.78      0.52      3380\n",
      "           2       0.89      0.62      0.73      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.55      0.53      0.50     13512\n",
      "weighted avg       0.72      0.62      0.63     13512\n",
      "\n",
      "KNN, Decision Tree, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.19      0.25      1262\n",
      "           1       0.43      0.78      0.56      3380\n",
      "           2       0.90      0.68      0.78      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.56      0.55      0.53     13512\n",
      "weighted avg       0.73      0.66      0.67     13512\n",
      "\n",
      "KNN, Decision Tree, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.05      0.09      1262\n",
      "           1       0.47      0.63      0.54      3380\n",
      "           2       0.81      0.80      0.81      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.56      0.50      0.48     13512\n",
      "weighted avg       0.69      0.69      0.67     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.10      0.15      1262\n",
      "           1       0.60      0.67      0.63      3380\n",
      "           2       0.84      0.89      0.86      8870\n",
      "\n",
      "    accuracy                           0.76     13512\n",
      "   macro avg       0.60      0.55      0.55     13512\n",
      "weighted avg       0.73      0.76      0.74     13512\n",
      "\n",
      "KNN, Random Forest, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.10      0.16      1262\n",
      "           1       0.41      0.79      0.54      3380\n",
      "           2       0.87      0.67      0.75      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.57      0.52      0.49     13512\n",
      "weighted avg       0.71      0.64      0.65     13512\n",
      "\n",
      "KNN, Random Forest, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.10      0.16      1262\n",
      "           1       0.46      0.78      0.57      3380\n",
      "           2       0.88      0.73      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.58      0.54      0.51     13512\n",
      "weighted avg       0.73      0.69      0.68     13512\n",
      "\n",
      "KNN, Random Forest, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.03      0.06      1262\n",
      "           1       0.54      0.60      0.57      3380\n",
      "           2       0.80      0.87      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.58      0.50      0.49     13512\n",
      "weighted avg       0.70      0.72      0.69     13512\n",
      "\n",
      "KNN, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.25      0.25      1262\n",
      "           1       0.34      0.68      0.45      3380\n",
      "           2       0.86      0.53      0.66      8870\n",
      "\n",
      "    accuracy                           0.54     13512\n",
      "   macro avg       0.48      0.49      0.45     13512\n",
      "weighted avg       0.67      0.54      0.57     13512\n",
      "\n",
      "KNN, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.28      0.28      1262\n",
      "           1       0.43      0.68      0.53      3380\n",
      "           2       0.88      0.68      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.53      0.55      0.52     13512\n",
      "weighted avg       0.71      0.64      0.66     13512\n",
      "\n",
      "KNN, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.08      0.12      1262\n",
      "           1       0.34      0.59      0.43      3380\n",
      "           2       0.78      0.63      0.70      8870\n",
      "\n",
      "    accuracy                           0.57     13512\n",
      "   macro avg       0.45      0.44      0.42     13512\n",
      "weighted avg       0.62      0.57      0.58     13512\n",
      "\n",
      "KNN, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.30      0.29      1262\n",
      "           1       0.39      0.75      0.51      3380\n",
      "           2       0.91      0.58      0.71      8870\n",
      "\n",
      "    accuracy                           0.59     13512\n",
      "   macro avg       0.53      0.54      0.50     13512\n",
      "weighted avg       0.72      0.59      0.62     13512\n",
      "\n",
      "KNN, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.08      0.12      1262\n",
      "           1       0.37      0.70      0.49      3380\n",
      "           2       0.83      0.63      0.72      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.48      0.47      0.44     13512\n",
      "weighted avg       0.66      0.60      0.60     13512\n",
      "\n",
      "KNN, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.09      0.13      1262\n",
      "           1       0.41      0.68      0.51      3380\n",
      "           2       0.83      0.70      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.50      0.49      0.47     13512\n",
      "weighted avg       0.67      0.64      0.64     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.12      0.19      1262\n",
      "           1       0.57      0.72      0.64      3380\n",
      "           2       0.86      0.85      0.85      8870\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.61      0.57      0.56     13512\n",
      "weighted avg       0.74      0.75      0.74     13512\n",
      "\n",
      "Decision Tree, Random Forest, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.13      0.20      1262\n",
      "           1       0.41      0.81      0.54      3380\n",
      "           2       0.89      0.64      0.74      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.57      0.53      0.50     13512\n",
      "weighted avg       0.72      0.64      0.64     13512\n",
      "\n",
      "Decision Tree, Random Forest, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.13      0.20      1262\n",
      "           1       0.44      0.81      0.57      3380\n",
      "           2       0.90      0.70      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.59      0.55      0.52     13512\n",
      "weighted avg       0.74      0.68      0.68     13512\n",
      "\n",
      "Decision Tree, Random Forest, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.04      0.08      1262\n",
      "           1       0.53      0.69      0.60      3380\n",
      "           2       0.82      0.83      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.62      0.52      0.50     13512\n",
      "weighted avg       0.72      0.72      0.70     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.29      0.27      1262\n",
      "           1       0.34      0.70      0.46      3380\n",
      "           2       0.88      0.51      0.65      8870\n",
      "\n",
      "    accuracy                           0.54     13512\n",
      "   macro avg       0.49      0.50      0.46     13512\n",
      "weighted avg       0.69      0.54      0.57     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Gradient Boosting:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.33      0.31      1262\n",
      "           1       0.43      0.73      0.54      3380\n",
      "           2       0.90      0.65      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.54      0.57      0.53     13512\n",
      "weighted avg       0.73      0.64      0.66     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.11      0.15      1262\n",
      "           1       0.43      0.59      0.50      3380\n",
      "           2       0.80      0.75      0.78      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.50      0.48      0.48     13512\n",
      "weighted avg       0.66      0.65      0.65     13512\n",
      "\n",
      "Decision Tree, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.34      0.31      1262\n",
      "           1       0.38      0.75      0.51      3380\n",
      "           2       0.92      0.56      0.70      8870\n",
      "\n",
      "    accuracy                           0.59     13512\n",
      "   macro avg       0.53      0.55      0.50     13512\n",
      "weighted avg       0.73      0.59      0.61     13512\n",
      "\n",
      "Decision Tree, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.10      0.15      1262\n",
      "           1       0.38      0.77      0.51      3380\n",
      "           2       0.86      0.61      0.71      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.50      0.49      0.46     13512\n",
      "weighted avg       0.69      0.60      0.61     13512\n",
      "\n",
      "Decision Tree, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.11      0.16      1262\n",
      "           1       0.42      0.78      0.54      3380\n",
      "           2       0.87      0.67      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.53      0.52      0.49     13512\n",
      "weighted avg       0.71      0.64      0.65     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.18      0.22      1262\n",
      "           1       0.39      0.70      0.50      3380\n",
      "           2       0.87      0.64      0.74      8870\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.51      0.51      0.49     13512\n",
      "weighted avg       0.69      0.61      0.63     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.18      0.23      1262\n",
      "           1       0.43      0.74      0.55      3380\n",
      "           2       0.88      0.70      0.78      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.55      0.54      0.52     13512\n",
      "weighted avg       0.72      0.66      0.67     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.07      0.12      1262\n",
      "           1       0.49      0.54      0.51      3380\n",
      "           2       0.78      0.83      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.52      0.48      0.48     13512\n",
      "weighted avg       0.66      0.69      0.67     13512\n",
      "\n",
      "Random Forest, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.21      0.25      1262\n",
      "           1       0.38      0.77      0.51      3380\n",
      "           2       0.90      0.59      0.71      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.53      0.52      0.49     13512\n",
      "weighted avg       0.72      0.60      0.62     13512\n",
      "\n",
      "Random Forest, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.07      0.12      1262\n",
      "           1       0.40      0.76      0.52      3380\n",
      "           2       0.85      0.65      0.74      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.53      0.49      0.46     13512\n",
      "weighted avg       0.69      0.62      0.62     13512\n",
      "\n",
      "Random Forest, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.07      0.12      1262\n",
      "           1       0.44      0.76      0.56      3380\n",
      "           2       0.86      0.72      0.78      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.56      0.52      0.48     13512\n",
      "weighted avg       0.71      0.67      0.66     13512\n",
      "\n",
      "Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.46      0.28      1262\n",
      "           1       0.39      0.67      0.49      3380\n",
      "           2       0.89      0.49      0.64      8870\n",
      "\n",
      "    accuracy                           0.53     13512\n",
      "   macro avg       0.50      0.54      0.47     13512\n",
      "weighted avg       0.70      0.53      0.57     13512\n",
      "\n",
      "Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.19      0.19      1262\n",
      "           1       0.34      0.64      0.44      3380\n",
      "           2       0.80      0.53      0.64      8870\n",
      "\n",
      "    accuracy                           0.52     13512\n",
      "   macro avg       0.44      0.45      0.42     13512\n",
      "weighted avg       0.63      0.52      0.55     13512\n",
      "\n",
      "Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.19      0.20      1262\n",
      "           1       0.35      0.64      0.45      3380\n",
      "           2       0.81      0.56      0.66      8870\n",
      "\n",
      "    accuracy                           0.54     13512\n",
      "   macro avg       0.45      0.46      0.44     13512\n",
      "weighted avg       0.64      0.54      0.56     13512\n",
      "\n",
      "Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.20      0.20      1262\n",
      "           1       0.40      0.79      0.53      3380\n",
      "           2       0.89      0.56      0.69      8870\n",
      "\n",
      "    accuracy                           0.58     13512\n",
      "   macro avg       0.50      0.52      0.47     13512\n",
      "weighted avg       0.70      0.58      0.60     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.08      0.14      1262\n",
      "           1       0.53      0.77      0.63      3380\n",
      "           2       0.87      0.82      0.84      8870\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.63      0.56      0.54     13512\n",
      "weighted avg       0.75      0.74      0.72     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.08      0.14      1262\n",
      "           1       0.52      0.77      0.62      3380\n",
      "           2       0.87      0.82      0.84      8870\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.62      0.56      0.54     13512\n",
      "weighted avg       0.75      0.74      0.72     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.10      0.17      1262\n",
      "           1       0.54      0.76      0.63      3380\n",
      "           2       0.87      0.83      0.85      8870\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.63      0.56      0.55     13512\n",
      "weighted avg       0.75      0.75      0.73     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.06      0.11      1262\n",
      "           1       0.56      0.75      0.64      3380\n",
      "           2       0.86      0.85      0.85      8870\n",
      "\n",
      "    accuracy                           0.75     13512\n",
      "   macro avg       0.64      0.55      0.53     13512\n",
      "weighted avg       0.75      0.75      0.73     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Adaptive Boosting:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.17      0.23      1262\n",
      "           1       0.41      0.79      0.54      3380\n",
      "           2       0.89      0.64      0.75      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.56      0.53      0.51     13512\n",
      "weighted avg       0.72      0.63      0.65     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.17      0.24      1262\n",
      "           1       0.42      0.81      0.56      3380\n",
      "           2       0.91      0.66      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.57      0.55      0.52     13512\n",
      "weighted avg       0.74      0.65      0.66     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.08      0.14      1262\n",
      "           1       0.45      0.68      0.54      3380\n",
      "           2       0.83      0.76      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.57      0.51      0.49     13512\n",
      "weighted avg       0.70      0.68      0.67     13512\n",
      "\n",
      "KNN, Decision Tree, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.18      0.24      1262\n",
      "           1       0.43      0.80      0.56      3380\n",
      "           2       0.91      0.68      0.78      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.57      0.55      0.53     13512\n",
      "weighted avg       0.74      0.66      0.67     13512\n",
      "\n",
      "KNN, Decision Tree, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.10      0.16      1262\n",
      "           1       0.45      0.68      0.54      3380\n",
      "           2       0.84      0.77      0.80      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.57      0.52      0.50     13512\n",
      "weighted avg       0.70      0.68      0.68     13512\n",
      "\n",
      "KNN, Decision Tree, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.13      0.19      1262\n",
      "           1       0.48      0.81      0.60      3380\n",
      "           2       0.89      0.75      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.60      0.56      0.54     13512\n",
      "weighted avg       0.74      0.71      0.70     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.10      0.15      1262\n",
      "           1       0.43      0.78      0.56      3380\n",
      "           2       0.87      0.70      0.77      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.57      0.52      0.49     13512\n",
      "weighted avg       0.72      0.66      0.66     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.10      0.16      1262\n",
      "           1       0.46      0.79      0.58      3380\n",
      "           2       0.88      0.74      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.59      0.54      0.51     13512\n",
      "weighted avg       0.73      0.69      0.69     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.05      0.09      1262\n",
      "           1       0.51      0.65      0.57      3380\n",
      "           2       0.81      0.83      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.59      0.51      0.50     13512\n",
      "weighted avg       0.70      0.71      0.69     13512\n",
      "\n",
      "KNN, Random Forest, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.10      0.16      1262\n",
      "           1       0.46      0.79      0.58      3380\n",
      "           2       0.88      0.74      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.59      0.54      0.52     13512\n",
      "weighted avg       0.73      0.69      0.69     13512\n",
      "\n",
      "KNN, Random Forest, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.07      0.12      1262\n",
      "           1       0.54      0.74      0.63      3380\n",
      "           2       0.85      0.84      0.84      8870\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.62      0.55      0.53     13512\n",
      "weighted avg       0.74      0.74      0.72     13512\n",
      "\n",
      "KNN, Random Forest, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.07      0.12      1262\n",
      "           1       0.55      0.74      0.63      3380\n",
      "           2       0.85      0.84      0.85      8870\n",
      "\n",
      "    accuracy                           0.74     13512\n",
      "   macro avg       0.62      0.55      0.53     13512\n",
      "weighted avg       0.74      0.74      0.73     13512\n",
      "\n",
      "KNN, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.29      0.29      1262\n",
      "           1       0.40      0.75      0.52      3380\n",
      "           2       0.91      0.59      0.72      8870\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.53      0.55      0.51     13512\n",
      "weighted avg       0.72      0.61      0.63     13512\n",
      "\n",
      "KNN, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.08      0.12      1262\n",
      "           1       0.39      0.69      0.50      3380\n",
      "           2       0.83      0.66      0.74      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.49      0.48      0.45     13512\n",
      "weighted avg       0.66      0.62      0.62     13512\n",
      "\n",
      "KNN, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.08      0.12      1262\n",
      "           1       0.41      0.72      0.52      3380\n",
      "           2       0.84      0.68      0.75      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.50      0.49      0.47     13512\n",
      "weighted avg       0.68      0.63      0.64     13512\n",
      "\n",
      "KNN, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.17      0.22      1262\n",
      "           1       0.42      0.68      0.52      3380\n",
      "           2       0.84      0.70      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.52      0.52      0.50     13512\n",
      "weighted avg       0.69      0.64      0.65     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.12      0.19      1262\n",
      "           1       0.42      0.81      0.56      3380\n",
      "           2       0.89      0.67      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.58      0.53      0.50     13512\n",
      "weighted avg       0.73      0.65      0.66     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.12      0.19      1262\n",
      "           1       0.45      0.82      0.58      3380\n",
      "           2       0.90      0.71      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.59      0.55      0.52     13512\n",
      "weighted avg       0.74      0.68      0.68     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.08      0.13      1262\n",
      "           1       0.50      0.72      0.59      3380\n",
      "           2       0.84      0.80      0.82      8870\n",
      "\n",
      "    accuracy                           0.71     13512\n",
      "   macro avg       0.62      0.53      0.51     13512\n",
      "weighted avg       0.72      0.71      0.70     13512\n",
      "\n",
      "Decision Tree, Random Forest, Adaptive Boosting, Gradient Boosting:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.13      0.20      1262\n",
      "           1       0.45      0.81      0.58      3380\n",
      "           2       0.90      0.71      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.59      0.55      0.52     13512\n",
      "weighted avg       0.74      0.68      0.68     13512\n",
      "\n",
      "Decision Tree, Random Forest, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.09      0.15      1262\n",
      "           1       0.52      0.77      0.62      3380\n",
      "           2       0.86      0.80      0.83      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.64      0.55      0.53     13512\n",
      "weighted avg       0.74      0.73      0.71     13512\n",
      "\n",
      "Decision Tree, Random Forest, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.09      0.16      1262\n",
      "           1       0.52      0.76      0.62      3380\n",
      "           2       0.86      0.81      0.83      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.64      0.56      0.54     13512\n",
      "weighted avg       0.75      0.73      0.72     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.32      0.30      1262\n",
      "           1       0.39      0.76      0.52      3380\n",
      "           2       0.92      0.57      0.71      8870\n",
      "\n",
      "    accuracy                           0.60     13512\n",
      "   macro avg       0.53      0.55      0.51     13512\n",
      "weighted avg       0.73      0.60      0.62     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.10      0.15      1262\n",
      "           1       0.40      0.76      0.52      3380\n",
      "           2       0.86      0.63      0.73      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.51      0.50      0.47     13512\n",
      "weighted avg       0.69      0.62      0.62     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.10      0.15      1262\n",
      "           1       0.41      0.80      0.54      3380\n",
      "           2       0.88      0.65      0.75      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.53      0.52      0.48     13512\n",
      "weighted avg       0.71      0.63      0.64     13512\n",
      "\n",
      "Decision Tree, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.22      0.26      1262\n",
      "           1       0.43      0.76      0.55      3380\n",
      "           2       0.89      0.66      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.54      0.55      0.52     13512\n",
      "weighted avg       0.72      0.65      0.66     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.17      0.22      1262\n",
      "           1       0.40      0.76      0.52      3380\n",
      "           2       0.88      0.64      0.74      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.54      0.52      0.50     13512\n",
      "weighted avg       0.71      0.63      0.64     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.11      0.17      1262\n",
      "           1       0.43      0.74      0.54      3380\n",
      "           2       0.84      0.70      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.56      0.51      0.49     13512\n",
      "weighted avg       0.70      0.65      0.65     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.11      0.17      1262\n",
      "           1       0.44      0.77      0.56      3380\n",
      "           2       0.86      0.71      0.78      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.58      0.53      0.50     13512\n",
      "weighted avg       0.72      0.67      0.67     13512\n",
      "\n",
      "Random Forest, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.12      0.19      1262\n",
      "           1       0.46      0.76      0.58      3380\n",
      "           2       0.86      0.74      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.59      0.54      0.52     13512\n",
      "weighted avg       0.72      0.69      0.69     13512\n",
      "\n",
      "Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.20      0.20      1262\n",
      "           1       0.40      0.80      0.53      3380\n",
      "           2       0.89      0.55      0.68      8870\n",
      "\n",
      "    accuracy                           0.58     13512\n",
      "   macro avg       0.50      0.52      0.47     13512\n",
      "weighted avg       0.71      0.58      0.60     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.14      0.20      1262\n",
      "           1       0.45      0.78      0.57      3380\n",
      "           2       0.88      0.72      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.58      0.54      0.52     13512\n",
      "weighted avg       0.73      0.68      0.68     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.14      0.21      1262\n",
      "           1       0.46      0.79      0.58      3380\n",
      "           2       0.88      0.73      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.59      0.55      0.53     13512\n",
      "weighted avg       0.74      0.69      0.69     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.06      0.11      1262\n",
      "           1       0.53      0.69      0.60      3380\n",
      "           2       0.83      0.84      0.83      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.60      0.53      0.51     13512\n",
      "weighted avg       0.72      0.73      0.71     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.16      0.23      1262\n",
      "           1       0.45      0.82      0.58      3380\n",
      "           2       0.90      0.71      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.59      0.56      0.53     13512\n",
      "weighted avg       0.74      0.68      0.69     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.07      0.12      1262\n",
      "           1       0.50      0.77      0.61      3380\n",
      "           2       0.86      0.79      0.83      8870\n",
      "\n",
      "    accuracy                           0.72     13512\n",
      "   macro avg       0.62      0.54      0.52     13512\n",
      "weighted avg       0.74      0.72      0.71     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.07      0.12      1262\n",
      "           1       0.51      0.77      0.62      3380\n",
      "           2       0.87      0.81      0.84      8870\n",
      "\n",
      "    accuracy                           0.73     13512\n",
      "   macro avg       0.62      0.55      0.53     13512\n",
      "weighted avg       0.74      0.73      0.71     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.26      0.28      1262\n",
      "           1       0.40      0.77      0.53      3380\n",
      "           2       0.90      0.61      0.73      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.53      0.54      0.51     13512\n",
      "weighted avg       0.72      0.62      0.63     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.12      0.17      1262\n",
      "           1       0.40      0.75      0.52      3380\n",
      "           2       0.86      0.65      0.74      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.53      0.51      0.48     13512\n",
      "weighted avg       0.70      0.63      0.63     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.12      0.18      1262\n",
      "           1       0.43      0.75      0.54      3380\n",
      "           2       0.86      0.70      0.77      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.55      0.52      0.50     13512\n",
      "weighted avg       0.71      0.66      0.66     13512\n",
      "\n",
      "KNN, Decision Tree, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.14      0.20      1262\n",
      "           1       0.43      0.80      0.56      3380\n",
      "           2       0.89      0.68      0.77      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.56      0.54      0.51     13512\n",
      "weighted avg       0.73      0.66      0.67     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.15      0.20      1262\n",
      "           1       0.40      0.78      0.53      3380\n",
      "           2       0.89      0.64      0.74      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.54      0.52      0.49     13512\n",
      "weighted avg       0.71      0.63      0.64     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.07      0.12      1262\n",
      "           1       0.41      0.73      0.53      3380\n",
      "           2       0.84      0.69      0.76      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.54      0.50      0.47     13512\n",
      "weighted avg       0.69      0.64      0.64     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.09      0.14      1262\n",
      "           1       0.45      0.72      0.55      3380\n",
      "           2       0.85      0.74      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.55      0.52      0.50     13512\n",
      "weighted avg       0.70      0.68      0.67     13512\n",
      "\n",
      "KNN, Random Forest, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.09      0.15      1262\n",
      "           1       0.45      0.79      0.57      3380\n",
      "           2       0.87      0.72      0.79      8870\n",
      "\n",
      "    accuracy                           0.68     13512\n",
      "   macro avg       0.57      0.53      0.50     13512\n",
      "weighted avg       0.72      0.68      0.67     13512\n",
      "\n",
      "KNN, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.20      0.22      1262\n",
      "           1       0.40      0.74      0.52      3380\n",
      "           2       0.87      0.62      0.72      8870\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.51      0.52      0.49     13512\n",
      "weighted avg       0.69      0.61      0.62     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.17      0.23      1262\n",
      "           1       0.40      0.79      0.53      3380\n",
      "           2       0.90      0.64      0.75      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.55      0.53      0.50     13512\n",
      "weighted avg       0.72      0.63      0.65     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.10      0.16      1262\n",
      "           1       0.41      0.76      0.54      3380\n",
      "           2       0.86      0.67      0.75      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.55      0.51      0.48     13512\n",
      "weighted avg       0.70      0.64      0.64     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.11      0.17      1262\n",
      "           1       0.44      0.76      0.56      3380\n",
      "           2       0.87      0.72      0.78      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.57      0.53      0.51     13512\n",
      "weighted avg       0.72      0.67      0.67     13512\n",
      "\n",
      "Decision Tree, Random Forest, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.12      0.19      1262\n",
      "           1       0.45      0.81      0.57      3380\n",
      "           2       0.89      0.70      0.78      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.58      0.54      0.52     13512\n",
      "weighted avg       0.73      0.67      0.68     13512\n",
      "\n",
      "Decision Tree, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.21      0.23      1262\n",
      "           1       0.40      0.77      0.53      3380\n",
      "           2       0.89      0.60      0.72      8870\n",
      "\n",
      "    accuracy                           0.61     13512\n",
      "   macro avg       0.51      0.53      0.49     13512\n",
      "weighted avg       0.71      0.61      0.62     13512\n",
      "\n",
      "Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.13      0.18      1262\n",
      "           1       0.39      0.77      0.52      3380\n",
      "           2       0.87      0.63      0.73      8870\n",
      "\n",
      "    accuracy                           0.62     13512\n",
      "   macro avg       0.54      0.51      0.48     13512\n",
      "weighted avg       0.70      0.62      0.63     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.14      0.21      1262\n",
      "           1       0.43      0.82      0.56      3380\n",
      "           2       0.89      0.67      0.77      8870\n",
      "\n",
      "    accuracy                           0.66     13512\n",
      "   macro avg       0.59      0.54      0.51     13512\n",
      "weighted avg       0.73      0.66      0.66     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.07      0.12      1262\n",
      "           1       0.44      0.78      0.57      3380\n",
      "           2       0.86      0.72      0.78      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.59      0.52      0.49     13512\n",
      "weighted avg       0.72      0.67      0.67     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.07      0.13      1262\n",
      "           1       0.46      0.79      0.59      3380\n",
      "           2       0.87      0.74      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.61      0.54      0.51     13512\n",
      "weighted avg       0.73      0.69      0.69     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.08      0.14      1262\n",
      "           1       0.46      0.79      0.58      3380\n",
      "           2       0.88      0.74      0.80      8870\n",
      "\n",
      "    accuracy                           0.69     13512\n",
      "   macro avg       0.61      0.54      0.51     13512\n",
      "weighted avg       0.74      0.69      0.69     13512\n",
      "\n",
      "KNN, Decision Tree, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.13      0.19      1262\n",
      "           1       0.41      0.82      0.54      3380\n",
      "           2       0.89      0.63      0.74      8870\n",
      "\n",
      "    accuracy                           0.63     13512\n",
      "   macro avg       0.55      0.53      0.49     13512\n",
      "weighted avg       0.72      0.63      0.64     13512\n",
      "\n",
      "KNN, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.09      0.14      1262\n",
      "           1       0.42      0.79      0.55      3380\n",
      "           2       0.87      0.67      0.76      8870\n",
      "\n",
      "    accuracy                           0.65     13512\n",
      "   macro avg       0.56      0.52      0.48     13512\n",
      "weighted avg       0.71      0.65      0.65     13512\n",
      "\n",
      "Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.11      0.18      1262\n",
      "           1       0.42      0.82      0.55      3380\n",
      "           2       0.89      0.65      0.75      8870\n",
      "\n",
      "    accuracy                           0.64     13512\n",
      "   macro avg       0.57      0.53      0.49     13512\n",
      "weighted avg       0.72      0.64      0.65     13512\n",
      "\n",
      "KNN, Decision Tree, Random Forest, Logistic Regression, Adaptive Boosting, Gradient Boosting, Gaussian Naiive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.10      0.17      1262\n",
      "           1       0.44      0.80      0.57      3380\n",
      "           2       0.88      0.71      0.79      8870\n",
      "\n",
      "    accuracy                           0.67     13512\n",
      "   macro avg       0.58      0.54      0.51     13512\n",
      "weighted avg       0.73      0.67      0.67     13512\n",
      "\n",
      "\n",
      "Best ensemble when fit to the original data (unbalanced) with optimized hyperparameters:\n",
      "\tKNN, Random Forest\n",
      "accuracy: 0.789890467732386\n"
     ]
    }
   ],
   "source": [
    "best_ensamble = None\n",
    "best_ensamble_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "for ensemble in ensembles:\n",
    "    ensemble_name = \", \".join([name for name in ensemble.keys()]) # join all the model names    \n",
    "    print(f\"{ensemble_name}:\")\n",
    "    \n",
    "    start = True\n",
    "    \n",
    "    sum_of_accs = np.sum(np.array([training_accuracies_iteration_4[name] for name in ensemble.keys()]))\n",
    "    preds = np.zeros(len(y_test))\n",
    "    for name, model in ensemble.items():\n",
    "        \n",
    "        pred = model.predict(X_test_scaled)\n",
    "        # multiply the prediction by the \"amount of say\" of this model\n",
    "        pred *= training_accuracies_iteration_4[name] / sum_of_accs\n",
    "        preds += pred\n",
    "        start = False\n",
    "    preds = (np.rint(preds)).astype(int)\n",
    "    \n",
    "    dict_report = classification_report(y_test, preds, output_dict=True)\n",
    "    str_report = classification_report(y_test, preds, output_dict=False)\n",
    "    print(str_report)\n",
    "    \n",
    "    if (dict_report[\"accuracy\"] > best_accuracy):\n",
    "        best_accuracy = dict_report[\"accuracy\"]\n",
    "        best_ensemble = models\n",
    "        best_ensemble_name = ensemble_name\n",
    "        \n",
    "print(f\"\\nBest ensemble when fit to the original data (unbalanced) with optimized hyperparameters:\\n\\t{best_ensemble_name}\")\n",
    "print(f\"accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857178f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
